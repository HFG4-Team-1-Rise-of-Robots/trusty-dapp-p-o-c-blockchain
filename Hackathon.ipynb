{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a69a7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import gensim\n",
    "import nltk as nl\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import os\n",
    "from nltk.stem import PorterStemmer \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff38fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:/Hackathon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0f316a",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f4d514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A little less than a decade ago, hockey fans w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The writers of the HBO series The Sopranos too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Despite claims from the TV news outlet to offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After receiving 'subpar' service and experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After watching his beloved Seattle Mariners pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  A little less than a decade ago, hockey fans w...\n",
       "1  The writers of the HBO series The Sopranos too...\n",
       "2  Despite claims from the TV news outlet to offe...\n",
       "3  After receiving 'subpar' service and experienc...\n",
       "4  After watching his beloved Seattle Mariners pr..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Headlines = pd.read_csv('buzzfeed-v02.csv', usecols =['Text'],sep=';').dropna()\n",
    "Headlines1 = pd.read_csv('fake_or_real_news.csv', usecols =['title'],sep=';').dropna()\n",
    "Headlines2 = pd.read_csv('UnreliableNewsData/full.csv', usecols =['Text'],sep=';').dropna()\n",
    "Headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d289029",
   "metadata": {},
   "outputs": [],
   "source": [
    "Headlines1 = Headlines1.drop_duplicates('title')\n",
    "Headlines = Headlines.drop_duplicates('Text')\n",
    "Headlines2 = Headlines2.drop_duplicates('Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c8fe97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Headlines1 = Headlines1.rename(columns={'title': 'headline_text'})\n",
    "Headlines = Headlines.rename(columns={'Text': 'headline_text'})\n",
    "Headlines2 = Headlines2.rename(columns={'Text': 'headline_text'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c1ed9",
   "metadata": {},
   "source": [
    "# Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc709cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating lable for datasets\n",
    "#buzfeed-headlines dataset will be used as real headlines\n",
    "#fake-or-real-news-dataset & unreliable-news dataset will be used as fake headlines\n",
    "Headlines['fake'] = 0\n",
    "Headlines1['fake'] = 1\n",
    "Headlines2['fake'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad75d969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset contains: 38720 Real headlines and 44977 Fake headlines.\n"
     ]
    }
   ],
   "source": [
    "data1 = pd.concat([Headlines,Headlines1,Headlines2]).astype(str)\n",
    "data = data1.copy()\n",
    "print('Training dataset contains: {} Real headlines and {} Fake headlines.'.format(len(Headlines),len(Headlines1)+len(Headlines2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7d691",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d7cfe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = nl.corpus.stopwords.words('english')\n",
    "gensim_stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
    "sklearn_stopwords = _stop_words.ENGLISH_STOP_WORDS\n",
    "combined_stopwords = sklearn_stopwords.union(nltk_stopwords,gensim_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a9272da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK has 179 stop words\n",
      "Gensim has 337 stop words\n",
      "Sklearn has 318 stop words\n",
      "Combined stopwords list has 390 stop words\n"
     ]
    }
   ],
   "source": [
    "print('NLTK has {} stop words'.format(len(nltk_stopwords)))\n",
    "print('Gensim has {} stop words'.format(len(gensim_stopwords)))\n",
    "print('Sklearn has {} stop words'.format(len(sklearn_stopwords)))\n",
    "print('Combined stopwords list has {} stop words'.format(len(combined_stopwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df277aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d8686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['headline_text'] = data['headline_text'].apply(lambda x: x.lower())\n",
    "data['headline_text'] = data['headline_text'].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))\n",
    "data['headline_text'] = data['headline_text'].apply(lambda x: ' '.join([porter_stemmer.stem(word) for word in x.split()]))\n",
    "data['headline_text'] = data['headline_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (combined_stopwords)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc9b1d",
   "metadata": {},
   "source": [
    "# Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4b6469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(data['headline_text'], data['fake'], test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c625f",
   "metadata": {},
   "source": [
    "# Construct models with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e7d87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, MaxPooling1D, Flatten, Embedding, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95679afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(tokenizer = word_tokenize, max_features = 300)\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(x_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(x_test)\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b81ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "svc = SVC(kernel='linear')\n",
    "knn = KNeighborsClassifier()\n",
    "nb = MultinomialNB()\n",
    "\n",
    "dt.fit(tfidf_train, y_train)\n",
    "rf.fit(tfidf_train, y_train)\n",
    "svc.fit(tfidf_train, y_train)\n",
    "knn.fit(tfidf_train, y_train)\n",
    "nb.fit(tfidf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a757cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Testing Acc. of Decision Tree: {} %\".format(round(dt.score(tfidf_test, y_test) * 100, 2)))\n",
    "print (\"Testing Acc. of Random Forest: {} %\".format(round(rf.score(tfidf_test, y_test) * 100, 2)))\n",
    "print (\"Testing Acc. of SVC: {} %\".format(round(svc.score(tfidf_test, y_test) * 100, 2)))\n",
    "print (\"Testing Acc. of K-NN: {} %\".format(round(knn.score(tfidf_test, y_test) * 100, 2)))\n",
    "print (\"Testing Acc. of Naive Bayesian: {} %\".format(round(nb.score(tfidf_test, y_test) * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c93f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train = tfidf_train.todense()\n",
    "tfidf_test = tfidf_test.todense()\n",
    "\n",
    "neural_network = Sequential()\n",
    "neural_network.add(Dense(64, input_dim=len(tfidf_features), activation='relu'))\n",
    "neural_network.add(Dropout(0.1))\n",
    "neural_network.add(Dense(64, activation='relu'))\n",
    "neural_network.add(Dropout(0.1))\n",
    "neural_network.add(Dense(1, activation='sigmoid'))\n",
    "neural_network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = neural_network.fit(tfidf_train, y_train, epochs=50, batch_size=512, verbose=0)\n",
    "_,test_acc = neural_network.evaluate(tfidf_test,y_test,verbose=0)\n",
    "print (\"Testing Acc. of DNN: {} %\".format(round(test_acc * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31303f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a117bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b09b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
